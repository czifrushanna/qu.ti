{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda3b8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nqc_test = create_qcbm_circuit(8, np.random.uniform(0, 2*np.pi, 16))\\nqc_test.measure_all()\\nresult_test = sampler.run([qc_test]).result()\\npub_result = result_test[0]\\nbit_array = pub_result.data.meas\\ncounts = dict(bit_array.get_counts())\\nprint(\"Counts sample:\")\\nfor k, v in list(counts.items())[:5]:\\n    print(f\"  Key: {k} (type: {type(k)}), Value: {v}\")\\nprint(f\"Total keys: {len(counts)}\")\\nprint(f\"Total shots: {sum(counts.values())}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the structure of the result object\n",
    "'''\n",
    "qc_test = create_qcbm_circuit(8, np.random.uniform(0, 2*np.pi, 16))\n",
    "qc_test.measure_all()\n",
    "result_test = sampler.run([qc_test]).result()\n",
    "pub_result = result_test[0]\n",
    "bit_array = pub_result.data.meas\n",
    "counts = dict(bit_array.get_counts())\n",
    "print(\"Counts sample:\")\n",
    "for k, v in list(counts.items())[:5]:\n",
    "    print(f\"  Key: {k} (type: {type(k)}), Value: {v}\")\n",
    "print(f\"Total keys: {len(counts)}\")\n",
    "print(f\"Total shots: {sum(counts.values())}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d993affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  core_composition ligand_composition  size (nm)  band_gap (eV)  \\\n",
      "0             InAs               TDPA        3.6           0.82   \n",
      "1             ZnSe      Zinc stearate        3.8           3.07   \n",
      "2             PbSe                MPA        6.9           1.06   \n",
      "3              ZnS     Hexadecylamine        2.3           3.98   \n",
      "4              InP                TOP        6.3           1.94   \n",
      "\n",
      "   quantum_yield (%)  emission_maximum (nm) lattice_type  a (Å)  b (Å)  c (Å)  \\\n",
      "0                 64                    904  Zinc Blende   6.06   6.06   6.06   \n",
      "1                 71                    544  Zinc Blende   5.67   5.67   5.67   \n",
      "2                 32                    905    Rock Salt   6.12   6.12   6.12   \n",
      "3                 45                    442        Cubic   5.41   5.41   5.41   \n",
      "4                 70                    634  Zinc Blende   5.87   5.87   5.87   \n",
      "\n",
      "   alpha (°)  beta (°)  gamma (°)  volume (Å³)  \n",
      "0       90.0      90.0       90.0       222.55  \n",
      "1       90.0      90.0       90.0       182.28  \n",
      "2       90.0      90.0       90.0       229.22  \n",
      "3       90.0      90.0       90.0       158.34  \n",
      "4       90.0      90.0       90.0       202.26  \n",
      "Shape: (100, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('syntheticQDstructure.csv')\n",
    "print(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86623d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   size (nm)  band_gap (eV)  quantum_yield (%)  emission_maximum (nm)  a (Å)  \\\n",
      "0        3.6           0.82                 64                    904   6.06   \n",
      "1        3.8           3.07                 71                    544   5.67   \n",
      "2        6.9           1.06                 32                    905   6.12   \n",
      "3        2.3           3.98                 45                    442   5.41   \n",
      "4        6.3           1.94                 70                    634   5.87   \n",
      "\n",
      "   b (Å)  c (Å)  alpha (°)  beta (°)  gamma (°)  ...  ligand_composition_OLA  \\\n",
      "0   6.06   6.06       90.0      90.0       90.0  ...                   False   \n",
      "1   5.67   5.67       90.0      90.0       90.0  ...                   False   \n",
      "2   6.12   6.12       90.0      90.0       90.0  ...                   False   \n",
      "3   5.41   5.41       90.0      90.0       90.0  ...                   False   \n",
      "4   5.87   5.87       90.0      90.0       90.0  ...                   False   \n",
      "\n",
      "   ligand_composition_Oleic acid  ligand_composition_TDPA  \\\n",
      "0                          False                     True   \n",
      "1                          False                    False   \n",
      "2                          False                    False   \n",
      "3                          False                    False   \n",
      "4                          False                    False   \n",
      "\n",
      "   ligand_composition_TOP  ligand_composition_TOPO  \\\n",
      "0                   False                    False   \n",
      "1                   False                    False   \n",
      "2                   False                    False   \n",
      "3                   False                    False   \n",
      "4                    True                    False   \n",
      "\n",
      "   ligand_composition_Zinc stearate  lattice_type_Cubic  \\\n",
      "0                             False               False   \n",
      "1                              True               False   \n",
      "2                             False               False   \n",
      "3                             False                True   \n",
      "4                             False               False   \n",
      "\n",
      "   lattice_type_Hexagonal  lattice_type_Rock Salt  lattice_type_Zinc Blende  \n",
      "0                   False                   False                      True  \n",
      "1                   False                   False                      True  \n",
      "2                   False                    True                     False  \n",
      "3                   False                   False                     False  \n",
      "4                   False                   False                      True  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "Encoded Shape: (100, 34)\n"
     ]
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(df)\n",
    "print(df_encoded.head())\n",
    "print(\"Encoded Shape:\", df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54547e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_array = df_encoded.to_numpy(dtype='float32')\n",
    "X_tensor = torch.tensor(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a075e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 61518.1465\n",
      "Epoch 2, Loss: 55954.9141\n",
      "Epoch 3, Loss: 34796.3447\n",
      "Epoch 4, Loss: 13436.8011\n",
      "Epoch 5, Loss: 6141.8463\n",
      "Epoch 6, Loss: 3758.0370\n",
      "Epoch 7, Loss: 3214.3361\n",
      "Epoch 8, Loss: 3405.9587\n",
      "Epoch 9, Loss: 1640.1172\n",
      "Epoch 10, Loss: 1221.8106\n",
      "Epoch 11, Loss: 1281.4724\n",
      "Epoch 12, Loss: 997.4612\n",
      "Epoch 13, Loss: 861.8401\n",
      "Epoch 14, Loss: 749.1600\n",
      "Epoch 15, Loss: 751.9536\n",
      "Epoch 16, Loss: 738.7723\n",
      "Epoch 17, Loss: 717.8904\n",
      "Epoch 18, Loss: 692.0080\n",
      "Epoch 19, Loss: 633.4734\n",
      "Epoch 20, Loss: 583.0721\n",
      "Epoch 21, Loss: 630.5296\n",
      "Epoch 22, Loss: 598.0688\n",
      "Epoch 23, Loss: 620.1712\n",
      "Epoch 24, Loss: 604.3057\n",
      "Epoch 25, Loss: 633.6089\n",
      "Epoch 26, Loss: 550.0746\n",
      "Epoch 27, Loss: 526.4237\n",
      "Epoch 28, Loss: 518.5535\n",
      "Epoch 29, Loss: 526.1904\n",
      "Epoch 30, Loss: 577.2879\n",
      "Epoch 31, Loss: 526.5234\n",
      "Epoch 32, Loss: 565.1510\n",
      "Epoch 33, Loss: 578.0210\n",
      "Epoch 34, Loss: 527.5343\n",
      "Epoch 35, Loss: 558.4906\n",
      "Epoch 36, Loss: 587.6406\n",
      "Epoch 37, Loss: 539.9879\n",
      "Epoch 38, Loss: 543.2601\n",
      "Epoch 39, Loss: 519.0472\n",
      "Epoch 40, Loss: 518.5500\n",
      "Epoch 41, Loss: 502.5959\n",
      "Epoch 42, Loss: 532.8985\n",
      "Epoch 43, Loss: 474.2290\n",
      "Epoch 44, Loss: 547.5166\n",
      "Epoch 45, Loss: 509.8457\n",
      "Epoch 46, Loss: 511.4333\n",
      "Epoch 47, Loss: 531.8124\n",
      "Epoch 48, Loss: 528.6434\n",
      "Epoch 49, Loss: 518.2852\n",
      "Epoch 50, Loss: 493.2912\n",
      "Encoded Data Shape: (100, 2)\n",
      "Encoded Data Sample: [[-408.29056 -340.8583 ]\n",
      " [-261.3248  -211.90912]\n",
      " [-409.06863 -342.03375]\n",
      " [-217.016   -174.78508]\n",
      " [-299.77014 -244.78973]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "# Define model, loss, optimizer\n",
    "input_dim = X_tensor.shape[1]\n",
    "latent_dim = 2  # small latent space for quantum training\n",
    "model = Autoencoder(input_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train autoencoder\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch[0])\n",
    "        loss = criterion(recon, batch[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save encoded data\n",
    "with torch.no_grad():\n",
    "    encoded_data = model.encoder(X_tensor).numpy()\n",
    "    \n",
    "print(\"Encoded Data Shape:\", encoded_data.shape)\n",
    "print(\"Encoded Data Sample:\", encoded_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c413ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "encoded_normalized = scaler.fit_transform(encoded_data)  # shape: (N, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3fa8174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitstrings Sample: ['00100010', '10111100', '00100010', '11101110', '10011001']\n"
     ]
    }
   ],
   "source": [
    "def float_to_bits(x, n_bits=4):\n",
    "    \"\"\"Convert a float in [0,1] to fixed-length binary string\"\"\"\n",
    "    max_int = 2**n_bits - 1\n",
    "    int_val = int(x * max_int)\n",
    "    return format(int_val, f'0{n_bits}b')\n",
    "\n",
    "n_bits = 4  # bits per latent dimension\n",
    "bitstrings = []\n",
    "\n",
    "for point in encoded_normalized:\n",
    "    bits = ''.join(float_to_bits(x, n_bits) for x in point)\n",
    "    bitstrings.append(bits)\n",
    "\n",
    "print(\"Bitstrings Sample:\", bitstrings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4abf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "import numpy as np\n",
    "\n",
    "def create_qcbm_circuit(num_qubits, parameters):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    idx = 0\n",
    "    for i in range(num_qubits):\n",
    "        qc.ry(parameters[idx], i)\n",
    "        idx += 1\n",
    "        qc.rz(parameters[idx], i)\n",
    "        idx += 1\n",
    "    for i in range(num_qubits - 1):\n",
    "        qc.cx(i, i + 1)\n",
    "    return qc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56c5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import StatevectorSampler\n",
    "from scipy.optimize import minimize\n",
    "from collections import Counter\n",
    "\n",
    "sampler = StatevectorSampler()\n",
    "\n",
    "def compute_kl_divergence(p_data, p_model, epsilon=1e-8):\n",
    "    # Ensure support match\n",
    "    keys = set(p_data.keys()).union(set(p_model.keys()))\n",
    "    return sum(\n",
    "        p_data.get(k, 0) * np.log((p_data.get(k, epsilon) + epsilon) / (p_model.get(k, epsilon) + epsilon))\n",
    "        for k in keys\n",
    "    )\n",
    "\n",
    "def evaluate_qcbm_loss(parameters, num_qubits, target_distribution):\n",
    "    qc = create_qcbm_circuit(num_qubits, parameters)\n",
    "    # Add measurements\n",
    "    qc.measure_all()\n",
    "    result = sampler.run([qc]).result()\n",
    "    # In Qiskit 2.x, the result format is different\n",
    "    pub_result = result[0]  # Get the first (and only) pub result\n",
    "    bit_array = pub_result.data.meas\n",
    "    counts = dict(bit_array.get_counts())\n",
    "    # Normalize output - keys are already strings\n",
    "    total_shots = sum(counts.values())\n",
    "    model_distribution = {k: v / total_shots for k, v in counts.items()}\n",
    "    return compute_kl_divergence(target_distribution, model_distribution)\n",
    "\n",
    "def compute_data_distribution(binary_encoded_data):\n",
    "    str_data = [''.join(str(bit) for bit in sample) for sample in binary_encoded_data]\n",
    "    counts = Counter(str_data)\n",
    "    total = sum(counts.values())\n",
    "    return {k: v / total for k, v in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed. Final loss: 5.338440470229853\n",
      "Number of qubits: 8\n",
      "Target distribution (first 5 items): {'00100010': 0.14, '10111100': 0.01, '11101110': 0.11, '10011001': 0.05, '11011100': 0.04}\n"
     ]
    }
   ],
   "source": [
    "# Convert bitstrings to binary array format\n",
    "binary_encoded_data = np.array([[int(bit) for bit in bitstring] for bitstring in bitstrings])\n",
    "\n",
    "num_qubits = binary_encoded_data.shape[1]\n",
    "num_parameters = 2 * num_qubits\n",
    "initial_params = np.random.uniform(0, 2 * np.pi, num_parameters)\n",
    "\n",
    "target_distribution = compute_data_distribution(binary_encoded_data)\n",
    "\n",
    "result = minimize(\n",
    "    evaluate_qcbm_loss,\n",
    "    x0=initial_params,\n",
    "    args=(num_qubits, target_distribution),\n",
    "    method='COBYLA',\n",
    "    options={'maxiter': 100}\n",
    ")\n",
    "\n",
    "trained_params = result.x\n",
    "print(f\"Optimization completed. Final loss: {result.fun}\")\n",
    "print(f\"Number of qubits: {num_qubits}\")\n",
    "print(f\"Target distribution (first 5 items): {dict(list(target_distribution.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b340d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most probable samples:\n",
      "1. 01001010: 0.0620\n",
      "2. 10101010: 0.0550\n",
      "3. 10110010: 0.0510\n",
      "4. 01011010: 0.0450\n",
      "5. 10100010: 0.0410\n",
      "6. 01000010: 0.0370\n",
      "7. 10111010: 0.0360\n",
      "8. 01010010: 0.0330\n",
      "9. 11001010: 0.0300\n",
      "10. 01010100: 0.0290\n"
     ]
    }
   ],
   "source": [
    "qc_trained = create_qcbm_circuit(num_qubits, trained_params)\n",
    "qc_trained.measure_all()\n",
    "result = sampler.run([qc_trained], shots=1000).result()\n",
    "\n",
    "# Get the counts and convert to distribution\n",
    "pub_result = result[0]\n",
    "bit_array = pub_result.data.meas\n",
    "counts = dict(bit_array.get_counts())\n",
    "total_shots = sum(counts.values())\n",
    "samples = {k: v / total_shots for k, v in counts.items()}\n",
    "\n",
    "# Sort samples by probability\n",
    "sorted_samples = sorted(samples.items(), key=lambda x: x[1], reverse=True)\n",
    "top_samples = [k for k, _ in sorted_samples[:10]]\n",
    "decoded_inputs = [np.array([int(bit) for bit in s]) for s in top_samples]\n",
    "\n",
    "print(\"Top 10 most probable samples:\")\n",
    "for i, (sample, prob) in enumerate(sorted_samples[:10]):\n",
    "    print(f\"{i+1}. {sample}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc7a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic results shape: (10, 34)\n",
      "Synthetic results sample: [[ 1.29270542e+00 -7.00705230e-01  6.47788696e+01  7.61845093e+02\n",
      "   7.79755783e+00 -2.72967982e+00 -1.18559372e+00  1.05435463e+02\n",
      "   9.10030060e+01  9.95890350e+01  2.15124298e+02 -2.14780375e-01\n",
      "  -7.76464319e+00 -8.91425133e+00 -1.23014545e+00 -1.41232061e+01\n",
      "  -1.05889091e+01 -1.20804894e+00 -7.34146118e-01  7.86216497e+00\n",
      "   1.04454031e+01 -1.02548437e+01 -3.01259637e+00 -1.13429985e+01\n",
      "  -8.28905964e+00 -2.06592226e+00 -2.17754936e+00  7.17689085e+00\n",
      "  -1.30619125e+01 -8.39052486e+00  1.21902103e+01  5.36464071e+00\n",
      "   1.37099111e+00  2.17105923e+01]\n",
      " [ 2.06064343e+00  9.01140690e-01  4.58947754e+01  6.35134399e+02\n",
      "   3.91183090e+00  3.96530056e+00  3.60622859e+00  7.98957825e+01\n",
      "   7.86123199e+01  8.30608978e+01  1.70213974e+02  1.13342226e-01\n",
      "   1.26197860e-02 -1.27786398e-01  2.27783918e-02  7.89934397e-01\n",
      "  -3.53521168e-01 -2.81317905e-02  1.01453662e-01  1.01163960e+00\n",
      "  -2.53309846e-01 -1.04592597e+00 -1.33004487e-01  4.53826129e-01\n",
      "  -2.63579488e-01 -2.89160162e-02 -4.66162786e-02  2.38533244e-01\n",
      "  -3.96917343e-01 -3.58107954e-01  4.08343762e-01  6.43139798e-03\n",
      "   3.50574851e-01  1.25223792e+00]\n",
      " [ 4.65020657e+00  9.24708366e+00  3.92823029e+01  7.26138855e+02\n",
      "  -5.08221769e+00  1.41824169e+01  6.09012222e+00  7.61610184e+01\n",
      "   9.37819977e+01  9.51703796e+01  1.85937042e+02 -5.80031490e+00\n",
      "   9.53264523e+00  1.30338173e+01  1.06552057e+01  2.05516739e+01\n",
      "   1.83837719e+01 -1.87058473e+00  6.43693113e+00 -7.87200308e+00\n",
      "  -1.28315821e+01  9.63631344e+00  6.09552050e+00  1.37491102e+01\n",
      "   1.12430029e+01 -4.30420923e+00  3.60497808e+00 -8.51034546e+00\n",
      "   1.53793221e+01  2.18124390e+01 -1.55476418e+01 -1.86063731e+00\n",
      "  -5.32134962e+00 -2.36640720e+01]]\n",
      "Latent vectors shape: torch.Size([10, 2])\n",
      "Latent vectors sample: tensor([[-388.6817, -240.9505],\n",
      "        [-293.0679, -240.9505],\n",
      "        [-277.1323, -352.1530]])\n"
     ]
    }
   ],
   "source": [
    "# Convert bit strings back to 2D latent space format\n",
    "def bits_to_float(bits, n_bits=4):\n",
    "    \"\"\"Convert binary string to float in [0,1]\"\"\"\n",
    "    max_int = 2**n_bits - 1\n",
    "    int_val = int(bits, 2)\n",
    "    return int_val / max_int\n",
    "\n",
    "# Convert the top samples back to 2D latent vectors\n",
    "latent_vectors = []\n",
    "for sample in top_samples:\n",
    "    # Split the 8-bit string into two 4-bit parts\n",
    "    bits_dim1 = sample[:4]  # First 4 bits\n",
    "    bits_dim2 = sample[4:]  # Last 4 bits\n",
    "    \n",
    "    # Convert back to floats in [0,1]\n",
    "    float_dim1 = bits_to_float(bits_dim1, n_bits)\n",
    "    float_dim2 = bits_to_float(bits_dim2, n_bits)\n",
    "    \n",
    "    latent_vectors.append([float_dim1, float_dim2])\n",
    "\n",
    "# Convert to tensor with proper scaling (inverse of normalization)\n",
    "latent_tensor = torch.tensor(latent_vectors, dtype=torch.float32)\n",
    "# Scale back to original latent space range using the fitted scaler\n",
    "latent_scaled = scaler.inverse_transform(latent_tensor.numpy())\n",
    "latent_scaled_tensor = torch.tensor(latent_scaled, dtype=torch.float32)\n",
    "\n",
    "model.decoder.eval()  # Set to eval mode (no dropout/batchnorm)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_outputs = model.decoder(latent_scaled_tensor)\n",
    "    \n",
    "synthetic_results = reconstructed_outputs.numpy()\n",
    "print(\"Synthetic results shape:\", synthetic_results.shape)\n",
    "print(\"Synthetic results sample:\", synthetic_results[:3])\n",
    "print(\"Latent vectors shape:\", latent_scaled_tensor.shape)\n",
    "print(\"Latent vectors sample:\", latent_scaled_tensor[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee5446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic DataFrame head:\n",
      "  core_composition ligand_composition  size (nm)  band_gap (eV)  \\\n",
      "0             ZnSe                DDA   1.292705      -0.700705   \n",
      "1             ZnSe                 OA   2.060643       0.901141   \n",
      "2              InP      Zinc stearate   4.650207       9.247084   \n",
      "3             ZnSe                DDA   1.363221      -1.201594   \n",
      "4              InP      Zinc stearate   4.479449       8.408709   \n",
      "\n",
      "   quantum_yield (%)  emission_maximum (nm) lattice_type     a (Å)      b (Å)  \\\n",
      "0          64.778870             761.845093  Zinc Blende  7.797558  -2.729680   \n",
      "1          45.894775             635.134399  Zinc Blende  3.911831   3.965301   \n",
      "2          39.282303             726.138855    Hexagonal -5.082218  14.182417   \n",
      "3          61.308781             740.915161  Zinc Blende  7.803060  -1.310134   \n",
      "4          42.189487             747.397583    Hexagonal -3.948589  13.292591   \n",
      "\n",
      "      c (Å)   alpha (°)   beta (°)  gamma (°)  volume (Å³)  \n",
      "0 -1.185594  105.435463  91.003006  99.589035   215.124298  \n",
      "1  3.606229   79.895782  78.612320  83.060898   170.213974  \n",
      "2  6.090122   76.161018  93.781998  95.170380   185.937042  \n",
      "3  0.549199  101.178841  89.126709  96.781548   206.421417  \n",
      "4  5.988115   80.417641  95.987610  97.885796   192.515884  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "categorical_cols = ['core_composition', 'ligand_composition', 'lattice_type']\n",
    "\n",
    "dummy_columns = [col for col in df_encoded.columns if any(col.startswith(cat + '_') for cat in categorical_cols)]\n",
    "synthetic_df = pd.DataFrame(synthetic_results, columns=df_encoded.columns)\n",
    "for col in categorical_cols:\n",
    "    # Extract relevant dummy columns\n",
    "    dummy_subset = synthetic_df[[c for c in synthetic_df.columns if c.startswith(col + '_')]]\n",
    "    \n",
    "    # Find argmax (i.e., the \"1\") to get back the original category\n",
    "    synthetic_df[col] = dummy_subset.idxmax(axis=1).str.replace(col + '_', '')\n",
    "\n",
    "    # Drop the dummy columns\n",
    "    synthetic_df.drop(columns=dummy_subset.columns, inplace=True)\n",
    "    \n",
    "column_order = df.columns.tolist()\n",
    "synthetic_df = synthetic_df.reindex(columns=column_order)\n",
    "\n",
    "print(\"Synthetic DataFrame head:\")\n",
    "print(synthetic_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ad123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv(\"synthetic_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
